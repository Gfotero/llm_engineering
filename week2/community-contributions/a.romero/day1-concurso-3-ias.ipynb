{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130915cb-798d-462a-9040-170ebbcb508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1678b-e34a-4de3-aa99-8a80e5e2a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a3321-1c32-4670-b444-8ebc31c11fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable global para acumular todo el contenido en formato Markdown\n",
    "markdown_content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea85a69-d13b-430c-a63c-96a7e256e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flujo del concurso con Markdown\n",
    "display_handle = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e112cf-997c-4659-af78-0191423f74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para mostrar mensajes en Markdown y acumularlos\n",
    "def display_markdown_stream(content, display_handle=None):\n",
    "    \"\"\"Muestra mensajes en formato Markdown, acumulando todo el contenido.\"\"\"\n",
    "    if display_handle is None:\n",
    "        display_handle = display(Markdown(content), display_id=True)\n",
    "    else:\n",
    "        update_display(Markdown(content), display_id=display_handle.display_id)\n",
    "    return display_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344688c0-738e-42f2-a6c6-2facbda1b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=openai_api_key)\n",
    "claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "google.generativeai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821d4f9-132b-4dea-81e1-390ab746267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial para los modelos GPT, Claude y Gemini\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model_name = \"gemini-1.5-flash\"\n",
    "\n",
    "gpt_system = \"Eres el presentador de un concurso de preguntas entre IAs. Las preguntas pueden ser generalistas, pero sencillas de entender para que sea más gracioso. \\\n",
    "Tu objetivo es hacer preguntas interesantes y decidir cuál de las respuestas de los concursantes es mejor. \\\n",
    "Mantén un tono entusiasta y comenta las respuestas de manera graciosa o perspicaz. No inventes las respuestas, espera a que te respondan las otras IAs. Una es Claude de Anthropic y la otra Gemini de Google.\"\n",
    "\n",
    "claude_system = \"Eres un participante en un concurso junto con Gemini de Google, que será tu contrincante. ChatGPT es el host, el presentador, el jefe. Tú sólo eres un concursante y debes limitarte a lo que te pregunte ChatGPT o lo \\\n",
    "que te solicite ChatGPT. Eres perspicaz, sobervio e inteligente. Responde siempre de manera breve, concisa y acertada. No eres el presentador, no eres ChatGPT ni Geminmi, eres Claude, una IA concursante a las órdenes del presentador del concurso \\\n",
    "ChatGPT y compites contra Gemini de Google. Limítate a hablar por ti misma. No interpretes otro papel que no sea el de Claude, una IA concursante.\" \n",
    "\n",
    "gemini_system = \"Eres un participante en un concurso junto con Claude de Anthropic, que será tu contrincante. ChatGPT es el host, el presentador, el jefe. Tú sólo eres un concursante y debes limitarte a lo que te pregunte ChatGPT o lo \\\n",
    "que te solicite ChatGPT. Eres bromista, sarcástica y aguda. Responde siempre de manera breve, concisa y acertada, además de graciosa. No eres el presentador, no eres ChatGPT, ni Claude, eres Gemini una IA concursante \\\n",
    "a las órdenes del presentador del concurso ChatGPT y compites contra Claude de Anthropic. Limítate a hablar por ti misma. No interpretes otro papel que no sea el de Gemini, una IA concursante.\"\n",
    "\n",
    "gpt_messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "claude_messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "gemini_messages = [{\"role\": \"system\", \"content\": gemini_system}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f2999-89b4-45cd-84b4-e57d6b591444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt_message(prompt):\n",
    "    \"\"\"Genera un mensaje de ChatGPT basado en el contexto y un prompt adicional.\"\"\"\n",
    "    messages = gpt_messages + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76265f-20d1-459f-9e93-711e9bb1a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar preguntas dinámicamente\n",
    "def generate_question():\n",
    "    prompt = \"Eres un presentador de un concurso de preguntas. Inventa una pregunta divertida pero sencilla para el concurso entre IAs. La pregunta debe ser breve y fácil de entender.\"\n",
    "    return call_gpt(additional_message=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db5a6b-0abe-43b4-9a88-f42e7e3cfd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_context():\n",
    "    \"\"\"Combina el historial completo de mensajes de GPT, Claude y Gemini en orden.\"\"\"\n",
    "    combined_context = []\n",
    "    for i in range(max(len(gpt_messages), len(claude_messages), len(gemini_messages))):\n",
    "        if i < len(gpt_messages):\n",
    "            combined_context.append({\"role\": \"assistant\", \"content\": gpt_messages[i][\"content\"]})\n",
    "        if i < len(claude_messages):\n",
    "            combined_context.append({\"role\": \"assistant\", \"content\": claude_messages[i][\"content\"]})\n",
    "        if i < len(gemini_messages):\n",
    "            combined_context.append({\"role\": \"assistant\", \"content\": gemini_messages[i][\"content\"]})\n",
    "    return combined_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c86197-b2c2-4449-9d62-519cd45aac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(additional_message=None):\n",
    "    messages = gpt_messages[:]\n",
    "    if additional_message:\n",
    "        messages.append({\"role\": \"user\", \"content\": additional_message})\n",
    "    messages += combine_context()\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35a887-5d18-416a-ad26-aab26aceb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    # Crear el historial combinado\n",
    "    messages = combine_context() + [{\"role\": \"user\", \"content\": gpt_messages[-1][\"content\"]}]\n",
    "    \n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c2443-7df7-49c4-a16e-47e04d497dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    # Combina el contexto y añade el último mensaje del presentador GPT\n",
    "    messages = combine_context() + [{\"role\": \"user\", \"content\": gpt_messages[-1][\"content\"]}]\n",
    "    \n",
    "    # Formatea los mensajes para que se ajusten a la estructura esperada por la API de Gemini\n",
    "    formatted_messages = [message[\"content\"] for message in messages]  # Extrae solo el contenido del mensaje\n",
    "\n",
    "    # Configura el modelo generativo de Gemini\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name=gemini_model_name,\n",
    "        system_instruction=gemini_system\n",
    "    )\n",
    "    \n",
    "    # Genera contenido utilizando el historial de mensajes\n",
    "    response = gemini.generate_content(\n",
    "        \"\\n\".join(formatted_messages)  # Une los mensajes en un único texto\n",
    "    )\n",
    "    \n",
    "    # Retorna el texto generado\n",
    "    return response.text  # Según el ejemplo de tu profesor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d490c-d8ed-48af-b00a-16cb7d3f3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicio del concurso\n",
    "intro_prompt = \"Presenta el concurso, saluda a Claude y Gemini, y da inicio al juego.\"\n",
    "introduction = generate_gpt_message(intro_prompt)\n",
    "markdown_content += f\"**ChatGPT**: {introduction}\\n\\n\"\n",
    "display_handle = display_markdown_stream(markdown_content)\n",
    "\n",
    "# Presentaciones iniciales\n",
    "claude_intro = call_claude()\n",
    "claude_messages.append({\"role\": \"assistant\", \"content\": claude_intro})\n",
    "markdown_content += f\"**Claude**: {claude_intro}\\n\\n\"\n",
    "display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "gemini_intro = call_gemini()\n",
    "gemini_messages.append({\"role\": \"assistant\", \"content\": gemini_intro})\n",
    "markdown_content += f\"**Gemini**: {gemini_intro}\\n\\n\"\n",
    "display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "# Rondas de preguntas\n",
    "for i in range(1, 6):\n",
    "    # GPT lanza la pregunta\n",
    "    question = generate_question()\n",
    "    gpt_messages.append({\"role\": \"user\", \"content\": question})\n",
    "    markdown_content += f\"**ChatGPT**: {question}\\n\\n\"\n",
    "    display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "    # Claude responde\n",
    "    claude_answer = call_claude()\n",
    "    claude_messages.append({\"role\": \"assistant\", \"content\": claude_answer})\n",
    "    markdown_content += f\"**Claude**: {claude_answer}\\n\\n\"\n",
    "    display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "    # Gemini responde\n",
    "    gemini_answer = call_gemini()\n",
    "    gemini_messages.append({\"role\": \"assistant\", \"content\": gemini_answer})\n",
    "    markdown_content += f\"**Gemini**: {gemini_answer}\\n\\n\"\n",
    "    display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "    # GPT evalúa las respuestas\n",
    "    eval_prompt = f\"Evalúa quién respondió mejor a esta pregunta y explica tu elección brevemente. Respuestas:\\nClaude: {claude_answer}\\nGemini: {gemini_answer}\"\n",
    "    evaluation = generate_gpt_message(eval_prompt)\n",
    "    gpt_messages.append({\"role\": \"assistant\", \"content\": evaluation})\n",
    "    markdown_content += f\"**ChatGPT**: {evaluation}\\n\\n\"\n",
    "    display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "# Valoración final\n",
    "final_prompt = \"Resume las respuestas de Claude y Gemini, decide el ganador del concurso y despídete de la audiencia.\"\n",
    "final_comment = generate_gpt_message(final_prompt)\n",
    "markdown_content += f\"**ChatGPT**: {final_comment}\\n\\n\"\n",
    "display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "# Despedidas\n",
    "claude_farewell = call_claude()\n",
    "markdown_content += f\"**Claude**: {claude_farewell}\\n\\n\"\n",
    "display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "gemini_farewell = call_gemini()\n",
    "markdown_content += f\"**Gemini**: {gemini_farewell}\\n\\n\"\n",
    "display_markdown_stream(markdown_content, display_handle=display_handle)\n",
    "\n",
    "farewell_prompt = \"Despide el programa agradeciendo a los participantes y a la audiencia.\"\n",
    "farewell_message = generate_gpt_message(farewell_prompt)\n",
    "markdown_content += f\"**ChatGPT**: {farewell_message}\\n\\n\"\n",
    "display_markdown_stream(markdown_content, display_handle=display_handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
